{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing required packages:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Get headers from https://httpbin.org/get , so that the website will allow me to access its data without assuming that I'm a robot:\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36 Edg/99.0.1150.30\", \n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-6227a395-53abbdc44198762105fbcd6d\"}\n",
    "\n",
    "# Creating a list:\n",
    "sectionlist = []\n",
    "\n",
    "# Creating a function where we can put any city and number of pages from the website:\n",
    "def getSections(city, page):\n",
    "    url = f'https://www.pararius.com/apartments/{city}/page-{page}'\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    lists = soup.find_all('li', {'class': 'search-list__item search-list__item--listing'}) # using findAll to find every single section (listing) the web page \n",
    "    print(len(lists))\n",
    "\n",
    "    # Loop over every section on the web page:\n",
    "    for list in lists:\n",
    "        section = {\n",
    "\n",
    "        # Get the 'name of the city', 'title of the apartment', 'location', 'price', 'area', 'number of rooms', and 'link to the listing' of the city in the Netherlands:  \n",
    "        'city' : city,\n",
    "        'title' : list.find('a', class_=\"listing-search-item__link--title\").text.replace('\\n', '').strip(),\n",
    "        'location' : list.find('div', class_=\"listing-search-item__location\").text.replace('\\n', '').strip(),\n",
    "        'price' : list.find('div', class_=\"listing-search-item__price\").text.replace('\\n', '').strip(),\n",
    "        'area' : list.find('li', class_=\"illustrated-features__item--surface-area\").text.replace('\\n', '') .strip(),\n",
    "        'rooms' : list.find('li', class_=\"illustrated-features__item illustrated-features__item--number-of-rooms\").text.replace('\\n', '').strip(),\n",
    "        'links' : \"https://www.pararius.com\" + list.find('a', class_=\"listing-search-item__link listing-search-item__link--title\") ['href']\n",
    "        } \n",
    "\n",
    "        # Searching for each item alone so we use 'find'\n",
    "        # Use \".text\": because we want the text of the HTML\n",
    "        # Replace('\\n', '') : replacing the extra '\\n' with white space\n",
    "        \n",
    "        # Append the listing section into a list:\n",
    "        sectionlist.append(section)\n",
    "    return\n",
    "\n",
    "# Loop over the pages:\n",
    "for x in range(1,34,1):\n",
    "  # extracting the data from three cities\n",
    "    print(f'Getting page: {x}')\n",
    "    getSections('amsterdam', x)\n",
    "    getSections('rotterdam', x)\n",
    "    getSections('amstelveen', x)\n",
    "    \n",
    "# Save listings in a data frame:\n",
    "df = pd.DataFrame(sectionlist)\n",
    "print(len(sectionlist))\n",
    "\n",
    "# Output the scraped data into an Excel file:\n",
    "df.to_excel('NetherlandsListings.xlsx', index=False)\n",
    "print('Scraping is done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
