{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeVyTW0Bd+UvU4f9EJfqLJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTFElP36ftod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23625a2e-30e4-4fac-a553-934363039a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting page: 1\n",
            "30\n",
            "30\n",
            "31\n",
            "Getting page: 2\n",
            "30\n",
            "30\n",
            "31\n",
            "Getting page: 3\n",
            "30\n",
            "30\n",
            "31\n",
            "Getting page: 4\n",
            "30\n",
            "30\n",
            "15\n",
            "Getting page: 5\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 6\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 7\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 8\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 9\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 10\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 11\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 12\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 13\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 14\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 15\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 16\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 17\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 18\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 19\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 20\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 21\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 22\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 23\n",
            "30\n",
            "30\n",
            "0\n",
            "Getting page: 24\n",
            "30\n",
            "21\n",
            "0\n",
            "Getting page: 25\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 26\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 27\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 28\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 29\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 30\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 31\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 32\n",
            "30\n",
            "0\n",
            "0\n",
            "Getting page: 33\n",
            "27\n",
            "0\n",
            "0\n",
            "1806\n",
            "Scraping is done\n"
          ]
        }
      ],
      "source": [
        "# Importing required packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# getting the headers from https://httpbin.org/get\n",
        "# so that the website will allow me to acces its data without assuming that I'm a robot\n",
        "headers = {\"User-Agent\": \"\"}\n",
        "# creating a list\n",
        "sectionlist = []\n",
        "\n",
        "# a function where we can put any city and number of pages in the website:\n",
        "def getSections(city, page):\n",
        "    url = f'https://www.pararius.com/apartments/{city}/page-{page}'\n",
        "    r = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    lists = soup.find_all('li', {'class': 'search-list__item search-list__item--listing'}) # using findAll to find every single section (listing) the web page \n",
        "    print(len(lists))\n",
        "    # loop over every section in the web page\n",
        "    for list in lists:\n",
        "        section = {\n",
        "        # get the 'name of the city', 'title of the apartment', 'location', 'price', 'area', 'number of rooms' and 'link to the selected listing' of a city in the Netherlands:\n",
        "        'city' : city,\n",
        "        'title' : list.find('a', class_=\"listing-search-item__link--title\").text.replace('\\n', '').strip(),\n",
        "        'location' : list.find('div', class_=\"listing-search-item__location\").text.replace('\\n', '').strip(),\n",
        "        'price' : list.find('div', class_=\"listing-search-item__price\").text.replace('\\n', '').strip(),\n",
        "        'area' : list.find('li', class_=\"illustrated-features__item--surface-area\").text.replace('\\n', '') .strip(),\n",
        "        'rooms' : list.find('li', class_=\"illustrated-features__item illustrated-features__item--number-of-rooms\").text.replace('\\n', '').strip(),\n",
        "        'links' : \"https://www.pararius.com\" + list.find('a', class_=\"listing-search-item__link listing-search-item__link--title\") ['href']\n",
        "        } \n",
        "        # find: searching for each item alone so we use 'find' \n",
        "        # .text : beacuse we want the text on the html\n",
        "        # replace('\\n', '') : replacing the extra '\\n' with white space\n",
        "        \n",
        "        # append the listing section into a list\n",
        "        sectionlist.append(section)\n",
        "    return\n",
        "# loop over the pages in the website:\n",
        "for x in range(1,34,1):\n",
        "  # extracting the data from three different cities in the Netherlands:\n",
        "    print(f'Getting page: {x}')\n",
        "    getSections('amsterdam', x)\n",
        "    getSections('rotterdam', x)\n",
        "    getSections('amstelveen', x)\n",
        "    \n",
        "# save listings into a dataframe:   \n",
        "df = pd.DataFrame(sectionlist)\n",
        "print(len(sectionlist))\n",
        "# output to an excel sheet:\n",
        "df.to_excel('NetherlandsListings.xlsx', index=False)\n",
        "print('Scraping is done')"
      ]
    }
  ]
}